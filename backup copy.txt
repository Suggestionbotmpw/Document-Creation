import torch
import json
import re
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from config import BASE_MODEL

# ======================================================
# App
# ======================================================
app = FastAPI(title="AI Recommendation Generator")

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ======================================================
# Load tokenizer (ONCE)
# ======================================================
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.pad_token = tokenizer.eos_token

# ======================================================
# Load base model + LoRA (ONCE)
# ======================================================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)

model = PeftModel.from_pretrained(base_model, "trained-lora")
model.eval()

# ======================================================
# Request Schema
# ======================================================
class GenerateRequest(BaseModel):
    Heading: str
    CustomerNeed: str
    Impact: str | None = ""
    ServicePerformed: str
    ServiceSummaryTitle: str
    ServiceSummaryContent: str
    Language: str = "en-US"

    NumRecommendations: int
    NumSupportingReasons: int
    NumEvidence: int


# ======================================================
# Helpers
# ======================================================
def extract_json(text):
    match = re.search(r"\{[\s\S]*\}", text)
    if not match:
        return None
    try:
        return json.loads(match.group())
    except json.JSONDecodeError:
        return None


def validate_output(data, n_rec, n_reason, n_ev):
    issues = []

    if data is None:
        return 0, ["Invalid JSON"]

    try:
        solutions = data["Solutions"]
        if len(solutions) != n_rec:
            issues.append("Recommendation count mismatch")

        for sol in solutions:
            reasons = sol.get("SupportingReasons", [])
            if len(reasons) != n_reason:
                issues.append("SupportingReason count mismatch")

            for r in reasons:
                ev = r.get("Evidence", [])
                if len(ev) != n_ev:
                    issues.append("Evidence count mismatch")

    except Exception:
        issues.append("Invalid structure")

    score = max(100 - (len(issues) * 15), 0)
    return score, list(set(issues))


# ======================================================
# API Endpoint
# ======================================================
@app.post("/generate")
def generate_document(req: GenerateRequest):
    prompt = f"""### Instruction:
Generate a professional technical recommendation document.

### Constraints (MANDATORY):
You MUST strictly follow these numeric requirements.
Violating them is incorrect output.

- NumRecommendations: {req.NumRecommendations}
- NumSupportingReasons: {req.NumSupportingReasons}
- NumEvidence: {req.NumEvidence}

### Input:
{json.dumps(req.dict(exclude={
    "NumRecommendations",
    "NumSupportingReasons",
    "NumEvidence"
}), indent=2)}

### Output (JSON ONLY — no extra text):
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=900,
            temperature=0.0,
            do_sample=False,
            repetition_penalty=1.15,
            eos_token_id=tokenizer.eos_token_id
        )

    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    result_json = extract_json(decoded)

    accuracy, issues = validate_output(
        result_json,
        req.NumRecommendations,
        req.NumSupportingReasons,
        req.NumEvidence
    )

    return {
        "generated_text": decoded,
        "parsed_json": result_json,
        "accuracy": accuracy,
        "issues": issues
    }




"Solutions": [  
    {     
        "Recommendation": "Implement a Mobil Serv Planned Engineering Service (PES) program for periodic inspections and recommendations",
        "Result": "to improve filtration practices and prompt identification of abnormal oil analysis results",
        "SupportingReasons": [{ 
            "SupportingReason": "Improve Oil Cleanliness through Filtration Practices",        
            "Evidence": [          
                {        
                    "EvidenceTitle": "Decoded HTML Revisiting our objective to reduce equipment failures due to lubricant contamination",
                    "EvidenceContent": "The current contamination levels are contributing to shortened oil lifeterms and increasing maintenance costs. Improving filtration practices will help reduce contaminants entering the oil reservoir and therefore keep oil cleaner longer; reducing both oil consumption and downtime."
                }
                ] },        
            {
            "SupportingReason": "Increase Oil Monitoring Frequency and Responsiveness",
            "Evidence": [
                {
                    "EvidenceTitle": "Decoded HTML Revisiting our objective to reduce equipment failures due to lubricant contamination",             
                    "EvidenceContent": "The current contamination levels are contributing to shortened oil lifeterms and increasing maintenance costs. Improving filtration practices will help reduce contaminants entering the oil reservoir and therefore keep oil cleaner longer; reducing both oil consumption and downtime."          
                }         
                ]       
                }    ]   },
                {      
                    "Recommendation": "Implement a Mobil Serv Planned Engineering Service (PES) program for periodic inspections and recommendations", 
                    "Result": "to improve filtration practices and prompt identification of abnormal oil analysis results",    
                    "SupportingReasons": [   
                        {          "SupportingReason": "Improve Oil Cleanliness through Filtration"}
                
                ]
                }
]


-------------------------------Working Well with 85% accuracy-----------------------------------------------------------------
import torch
import json
import re
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from difflib import SequenceMatcher
from config import BASE_MODEL


# ======================================================
# FastAPI App
# ======================================================
app = FastAPI(
    title="Interactive Recommendation Builder",
    version="1.5"
)


# ======================================================
# Load BASE MODEL ONLY
# ======================================================
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto"
)
model.eval()


# ======================================================
# INPUT SCHEMA
# ======================================================
class SupportingReasonInput(BaseModel):
    evidenceCount: int


class RecommendationInput(BaseModel):
    supportingReasons: List[SupportingReasonInput]


class GenerateRequest(BaseModel):
    context: dict
    recommendations: List[RecommendationInput]
    language: str = "en-US"


# ======================================================
# SANITIZER
# ======================================================
def sanitize(text: str) -> str:
    text = re.sub(
        r"^(recommendation|technical reason|supporting reason|justification|evidence paragraph|evidence statement)\s*:\s*",
        "",
        text.strip(),
        flags=re.IGNORECASE
    )
    text = re.sub(r"[\{\}\[\]]", "", text)
    text = re.sub(r"\b\d+\.\s*", "", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


# ======================================================
# SIMILARITY CHECK (CORE OF DE-DUP)
# ======================================================
def is_duplicate(text: str, previous: List[str], threshold: float = 0.85) -> bool:
    for prev in previous:
        ratio = SequenceMatcher(None, text.lower(), prev.lower()).ratio()
        if ratio >= threshold:
            return True
    return False


# ======================================================
# LLM CALL (TEXT ONLY)
# ======================================================
def generate_text(prompt: str, max_new_tokens: int = 180) -> str:
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_len = inputs["input_ids"].shape[1]

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            repetition_penalty=1.25,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.decode(output[0][input_len:], skip_special_tokens=True)
    return sanitize(text)


# ======================================================
# SAFE GENERATION (NO DUPLICATES)
# ======================================================
def generate_unique(prompt: str, previous: List[str], retries: int = 4) -> str:
    for _ in range(retries):
        text = generate_text(prompt)
        if text and not is_duplicate(text, previous):
            return text
    # fallback: return best effort
    return text


# ======================================================
# API ENDPOINT
# ======================================================
@app.post("/generate")
def generate_document(req: GenerateRequest):

    context_json = json.dumps(req.context, indent=2)
    final_output = {"Solutions": []}

    used_recommendations = []

    for rec in req.recommendations:

        # =========================
        # Recommendation
        # =========================
        rec_prompt = f"""
Write ONE professional technical recommendation sentence.

Rules:
- One sentence only
- Do NOT repeat previous recommendations
- Start directly with content
- No labels or formatting
- Language: {req.language}

Previous Recommendations:
{used_recommendations}

Context:
{context_json}
"""
        recommendation_text = generate_unique(rec_prompt, used_recommendations)
        used_recommendations.append(recommendation_text)

        solution = {
            "Recommendation": recommendation_text,
            "SupportingReasons": []
        }

        used_reasons = []

        for sr in rec.supportingReasons:

            # =========================
            # Supporting Reason
            # =========================
            sr_prompt = f"""
Write ONE concise technical justification sentence.

Rules:
- One sentence only
- Do NOT repeat previous reasons
- Focus on a DIFFERENT technical aspect
- No labels
- Language: {req.language}

Previous Supporting Reasons:
{used_reasons}

Recommendation:
{recommendation_text}

Context:
{context_json}
"""
            supporting_reason_text = generate_unique(sr_prompt, used_reasons)
            used_reasons.append(supporting_reason_text)

            used_evidence = []
            evidence_list = []

            for _ in range(sr.evidenceCount):

                # =========================
                # Evidence
                # =========================
                ev_prompt = f"""
Write 2–3 factual technical sentences as evidence.

Rules:
- Do NOT repeat previous evidence
- Use a DIFFERENT data point or observation
- No labels or conclusions
- Language: {req.language}

Previous Evidence:
{used_evidence}

Supporting Reason:
{supporting_reason_text}

Context:
{context_json}
"""
                evidence_text = generate_unique(ev_prompt, used_evidence, retries=5)
                used_evidence.append(evidence_text)

                evidence_list.append({
                    "EvidenceTitle": "",
                    "EvidenceContent": evidence_text
                })

            solution["SupportingReasons"].append({
                "SupportingReason": supporting_reason_text,
                "Evidence": evidence_list
            })

        final_output["Solutions"].append(solution)

    return final_output
